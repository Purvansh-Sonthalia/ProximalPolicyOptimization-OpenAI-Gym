{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete,Box,Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical,MultivariateNormal\n",
    "import os\n",
    "import cv2\n",
    "import pygame\n",
    "import threading\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "game_name = 'Acrobot-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[1.8459203]\n",
      "[ 0.23995811 -0.89873487 -3.5333352 ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(game_name,render_mode = 'human')\n",
    "print(isinstance(env.action_space,Box))\n",
    "print(env.action_space.sample())\n",
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Display:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.screen = None\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.is_running = False\n",
    "        self.display_thread = None\n",
    "\n",
    "        self.env.reset()\n",
    "        self.rendered_frame = self.env.render()\n",
    "\n",
    "    def initialize_display(self):\n",
    "        # Get the environment rendering size\n",
    "        render_size = self.env.render().shape[1::-1]\n",
    "\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode(render_size)\n",
    "        pygame.display.set_caption(\"Environment Display\")\n",
    "        self.is_running = True\n",
    "\n",
    "    def display_loop(self):\n",
    "        self.initialize_display()\n",
    "\n",
    "        while self.is_running:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    self.is_running = False\n",
    "\n",
    "            # Convert the frame to a Pygame surface\n",
    "            frame_surface = pygame.surfarray.make_surface(self.rendered_frame.swapaxes(0, 1))\n",
    "\n",
    "            # Display the frame on the screen\n",
    "            self.screen.blit(frame_surface, (0, 0))\n",
    "            pygame.display.flip()\n",
    "\n",
    "            # Limit the frame rate\n",
    "            self.clock.tick(30)\n",
    "\n",
    "        pygame.quit()\n",
    "\n",
    "    def start_display_thread(self):\n",
    "        self.display_thread = threading.Thread(target=self.display_loop)\n",
    "        self.display_thread.start()\n",
    "\n",
    "    def join_display_thread(self):\n",
    "        if self.display_thread is not None and self.display_thread.is_alive():\n",
    "            self.display_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "  def __init__(self, action_space, action_std_init=0.6):\n",
    "    super().__init__()\n",
    "\n",
    "    self.trained_for = 0\n",
    "    self.has_continuous_action_space = isinstance(action_space,Box)\n",
    "    self.action_space_size = action_space.shape[0] if self.has_continuous_action_space else 1\n",
    "    self.action_std_init = action_std_init\n",
    "    self.action_var = torch.full((self.action_space_size,), action_std_init * action_std_init).to(DEVICE)\n",
    "\n",
    "    self.low = action_space.low if self.has_continuous_action_space else action_space.start\n",
    "    self.high_low_dif = (action_space.high - action_space.low) if self.has_continuous_action_space else action_space.n\n",
    "\n",
    "\n",
    "    self.shared_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=8, stride=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "    self.policy_layers = nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, self.action_space_size),\n",
    "        nn.Sigmoid())\n",
    "    \n",
    "\n",
    "    self.value_layers = nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "        nn.Sigmoid())\n",
    "\n",
    "  def value(self, obs):\n",
    "    obs = obs.reshape(-1,3,400,400)\n",
    "    z = self.shared_layers(obs)\n",
    "    z = z.reshape(-1,64)\n",
    "    value = self.value_layers(z)\n",
    "    return value\n",
    "\n",
    "  def policy(self, obs):\n",
    "    obs=obs.reshape(-1,3,400,400)\n",
    "    z = self.shared_layers(obs)\n",
    "    z = z.reshape(-1,64)\n",
    "    policy_logits = self.policy_layers(z)\n",
    "    return policy_logits\n",
    "\n",
    "  def forward(self, obs):\n",
    "    obs=obs.reshape(-1,3,400,400)\n",
    "    z = self.shared_layers(obs)\n",
    "    z = z.reshape(-1,64)\n",
    "    policy_logits = self.policy_layers(z)\n",
    "    value = self.value_layers(z)\n",
    "    return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model,obs):\n",
    "    logits, val = model(obs)\n",
    "    if(not model.has_continuous_action_space):\n",
    "        act_distribution = Categorical(logits)\n",
    "        act = act_distribution.sample()\n",
    "        act_log_prob = act_distribution.log_prob(act)\n",
    "        act = act.item()\n",
    "        return act,act_log_prob.item(),val.item()\n",
    "    else:\n",
    "        cov_mat = torch.diag(model.action_var).unsqueeze(dim=0)\n",
    "        act_distribution = MultivariateNormal(logits,cov_mat)\n",
    "        act = act_distribution.sample()\n",
    "        act_log_prob = act_distribution.log_prob(act)\n",
    "        act = act.detach().cpu().numpy().flatten()\n",
    "        act = act*model.high_low_dif + np.ones_like(act)*model.low\n",
    "        return act,act_log_prob.detach().cpu().numpy().flatten(),val.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_actions(model,obs,acts):\n",
    "    logits, vals = model(obs)\n",
    "    if(model.has_continuous_action_space):\n",
    "        action_var = model.action_var.expand_as(logits)\n",
    "        cov_mat = torch.diag_embed(action_var).to(DEVICE)\n",
    "        act_distribution = MultivariateNormal(logits,cov_mat)\n",
    "        if(model.action_space_size == 1):\n",
    "            acts = acts.reshape(-1,model.action_space_size)\n",
    "    else:\n",
    "        act_distribution = Categorical(logits)\n",
    "    act_log_probs = act_distribution.log_prob(acts)\n",
    "    entropy = act_distribution.entropy()\n",
    "    \n",
    "    return act_log_probs,vals,entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "  def __init__(self,\n",
    "              actor_critic,\n",
    "              ppo_clip_val=0.2,\n",
    "              target_kl_div=0.01,\n",
    "              max_policy_train_iters=80,\n",
    "              value_train_iters=80,\n",
    "              policy_lr=3e-4,\n",
    "              value_lr=1e-2):\n",
    "    self.ac = actor_critic\n",
    "    self.ppo_clip_val = ppo_clip_val\n",
    "    self.target_kl_div = target_kl_div\n",
    "    self.max_policy_train_iters = max_policy_train_iters\n",
    "    self.value_train_iters = value_train_iters\n",
    "\n",
    "    policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "        list(self.ac.policy_layers.parameters())\n",
    "    self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "\n",
    "    value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "        list(self.ac.value_layers.parameters())\n",
    "    self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "  def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "    for _ in range(self.max_policy_train_iters):\n",
    "\n",
    "      log_probs, vals, entropy = evaluate_actions(self.ac,obs,acts)\n",
    "\n",
    "      policy_ratio = torch.exp(log_probs - old_log_probs)\n",
    "      clipped_ratio = policy_ratio.clamp(\n",
    "          1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "\n",
    "      clipped_loss = clipped_ratio * gaes\n",
    "      full_loss = policy_ratio * gaes\n",
    "      policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "\n",
    "      self.policy_optim.zero_grad()\n",
    "      policy_loss.backward()\n",
    "      self.policy_optim.step()\n",
    "\n",
    "      kl_div = (old_log_probs - log_probs).mean()\n",
    "      if kl_div >= self.target_kl_div:\n",
    "        break\n",
    "\n",
    "  def train_value(self, obs, returns):\n",
    "    for _ in range(self.value_train_iters):\n",
    "      self.value_optim.zero_grad()\n",
    "\n",
    "      values = self.ac.value(obs)\n",
    "      value_loss = (returns - values) ** 2\n",
    "      value_loss = value_loss.mean()\n",
    "\n",
    "      value_loss.backward()\n",
    "      self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and gamma param.\n",
    "    \"\"\"\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
    "    \"\"\"\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000,display=None):\n",
    "    \"\"\"\n",
    "    Performs a single rollout.\n",
    "    Returns training data in the shape (n_steps, observation_shape)\n",
    "    and the cumulative reward.\n",
    "    \"\"\"\n",
    "    ### Create data storage\n",
    "    train_data = [[], [], [], [], []] # obs, act, reward, values, act_log_probs\n",
    "    env.reset()\n",
    "    rgb_image = env.render()\n",
    "    resized_image = cv2.resize(rgb_image, (400, 400))\n",
    "    obs = torch.tensor(resized_image, dtype=torch.float32, device=DEVICE).permute(2, 0, 1)\n",
    "    obs = obs / 255.0 \n",
    "\n",
    "    ep_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        act,act_log_prob,val=select_action(model,obs)\n",
    "\n",
    "        next_obs, reward, done, _, _ = env.step(act)\n",
    "\n",
    "        for i, item in enumerate((obs.reshape(-1).cpu().numpy(), act, reward, val, act_log_prob)):\n",
    "          train_data[i].append(item)\n",
    "\n",
    "        rgb_image = env.render()\n",
    "        if(display != None):\n",
    "            display.rendered_frame = rgb_image\n",
    "        resized_image = cv2.resize(rgb_image, (400, 400))\n",
    "        obs = torch.tensor(resized_image, dtype=torch.float32, device=DEVICE).permute(2, 0, 1)\n",
    "        obs = obs / 255.0\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "\n",
    "    ### Do train data filtering\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(game_name,render_mode='rgb_array')\n",
    "model = ActorCriticNetwork(env.action_space)\n",
    "model = model.to(DEVICE)\n",
    "display = Display(env)\n",
    "\n",
    "train_data, reward = rollout(model, env,max_steps=1) # Test rollout function\n",
    "display.start_display_thread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training params\n",
    "n_episodes = 50000000\n",
    "print_freq = 1\n",
    "save_freq = 1\n",
    "max_steps = 100\n",
    "\n",
    "ppo = PPOTrainer(\n",
    "    model,\n",
    "    policy_lr = 3e-4,\n",
    "    value_lr = 1e-3,\n",
    "    target_kl_div = 0.02,\n",
    "    max_policy_train_iters = 40,\n",
    "    value_train_iters = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not found\n"
     ]
    }
   ],
   "source": [
    "if(os.path.isfile(f'{game_name}.pt')):\n",
    "    model.load_state_dict(torch.load(f'{game_name}.pt'))\n",
    "    print('loaded')\n",
    "else:\n",
    "    print('not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Avg Reward -916.8\n",
      "Episode 2 | Avg Reward -659.6\n",
      "Episode 3 | Avg Reward -823.3\n",
      "Episode 4 | Avg Reward -672.5\n",
      "Episode 5 | Avg Reward -841.5\n",
      "Episode 6 | Avg Reward -607.1\n",
      "Episode 7 | Avg Reward -622.9\n",
      "Episode 8 | Avg Reward -847.2\n",
      "Episode 9 | Avg Reward -796.6\n",
      "Episode 10 | Avg Reward -688.4\n",
      "Episode 11 | Avg Reward -816.3\n"
     ]
    }
   ],
   "source": [
    "ep_rewards = []\n",
    "for episode_idx in range(n_episodes):\n",
    "  # Perform rollout\n",
    "  train_data, reward = rollout(model, env, max_steps=max_steps,display=display)\n",
    "  ep_rewards.append(reward)\n",
    "\n",
    "  # Shuffle\n",
    "  permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "\n",
    "  # Policy data\n",
    "  obs = torch.tensor(train_data[0][permute_idxs],\n",
    "                     dtype=torch.float32, device=DEVICE)\n",
    "  acts = torch.tensor(train_data[1][permute_idxs],\n",
    "                      dtype=torch.int32, device=DEVICE)\n",
    "  gaes = torch.tensor(train_data[3][permute_idxs],\n",
    "                      dtype=torch.float32, device=DEVICE)\n",
    "  act_log_probs = torch.tensor(train_data[4][permute_idxs],\n",
    "                               dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "  # Value data\n",
    "  returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "  returns = (returns - returns.mean()) / (returns.std() + 1e-7)\n",
    "  returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "  # Train model\n",
    "  ppo.train_policy(obs, acts, act_log_probs, gaes)\n",
    "  ppo.train_value(obs, returns)\n",
    "\n",
    "  model.trained_for += 1\n",
    "\n",
    "  if (episode_idx + 1) % save_freq == 0:\n",
    "    torch.save(model.state_dict(), f'{game_name}.pt')\n",
    "\n",
    "  if (episode_idx + 1) % print_freq == 0:\n",
    "    print('Episode {} | Avg Reward {:.1f}'.format(\n",
    "        episode_idx + 1, np.mean(ep_rewards[-print_freq:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "env = gym.make(game_name,render_mode='human')\n",
    "model.load_state_dict(torch.load(f'{game_name}.pt'))\n",
    "print(f'Trained for {model.trained_for}')\n",
    "while(True):\n",
    "  # Perform rollout\n",
    "  train_data, reward = rollout(model, env,display=display)\n",
    "  env.render()\n",
    "\n",
    "  print('Episode {} | Avg Reward {:.1f}'.format(\n",
    "        episode_idx + 1, reward))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
