{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete,Box,Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import os\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "game_name = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n",
      "[ 1.4343741e+00 -3.8181785e-03  3.8957295e+00 -3.2106454e+00\n",
      "  2.6929338e+00 -1.7445391e-01  3.5152283e-01  6.0487342e-01]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(game_name,render_mode = 'human')\n",
    "print(isinstance(env.action_space,Box))\n",
    "print(env.action_space.sample())\n",
    "print(env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "  def __init__(self, obs_space_size, action_space):\n",
    "    super().__init__()\n",
    "\n",
    "    self.trained_for = 0\n",
    "    action_space_size = action_space.shape[0] if isinstance(action_space,Box) else action_space.n\n",
    "\n",
    "    self.shared_layers = nn.Sequential(\n",
    "        nn.Linear(obs_space_size, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU())\n",
    "\n",
    "    self.policy_layers = nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, action_space_size))\n",
    "\n",
    "    self.value_layers = nn.Sequential(\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1))\n",
    "\n",
    "  def value(self, obs):\n",
    "    z = self.shared_layers(obs)\n",
    "    value = self.value_layers(z)\n",
    "    return value\n",
    "\n",
    "  def policy(self, obs):\n",
    "    z = self.shared_layers(obs)\n",
    "    policy_logits = self.policy_layers(z)\n",
    "    return policy_logits\n",
    "\n",
    "  def forward(self, obs):\n",
    "    z = self.shared_layers(obs)\n",
    "    policy_logits = self.policy_layers(z)\n",
    "    value = self.value_layers(z)\n",
    "    return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer():\n",
    "  def __init__(self,\n",
    "              actor_critic,\n",
    "              ppo_clip_val=0.2,\n",
    "              target_kl_div=0.01,\n",
    "              max_policy_train_iters=80,\n",
    "              value_train_iters=80,\n",
    "              policy_lr=3e-4,\n",
    "              value_lr=1e-2):\n",
    "    self.ac = actor_critic\n",
    "    self.ppo_clip_val = ppo_clip_val\n",
    "    self.target_kl_div = target_kl_div\n",
    "    self.max_policy_train_iters = max_policy_train_iters\n",
    "    self.value_train_iters = value_train_iters\n",
    "\n",
    "    policy_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "        list(self.ac.policy_layers.parameters())\n",
    "    self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "\n",
    "    value_params = list(self.ac.shared_layers.parameters()) + \\\n",
    "        list(self.ac.value_layers.parameters())\n",
    "    self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "  def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "    for _ in range(self.max_policy_train_iters):\n",
    "      self.policy_optim.zero_grad()\n",
    "\n",
    "      new_logits = self.ac.policy(obs)\n",
    "      new_logits = Categorical(logits=new_logits)\n",
    "      new_log_probs = new_logits.log_prob(acts)\n",
    "\n",
    "      policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "      clipped_ratio = policy_ratio.clamp(\n",
    "          1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "\n",
    "      clipped_loss = clipped_ratio * gaes\n",
    "      full_loss = policy_ratio * gaes\n",
    "      policy_loss = -torch.min(full_loss, clipped_loss).mean()\n",
    "\n",
    "      policy_loss.backward()\n",
    "      self.policy_optim.step()\n",
    "\n",
    "      kl_div = (old_log_probs - new_log_probs).mean()\n",
    "      if kl_div >= self.target_kl_div:\n",
    "        break\n",
    "\n",
    "  def train_value(self, obs, returns):\n",
    "    for _ in range(self.value_train_iters):\n",
    "      self.value_optim.zero_grad()\n",
    "\n",
    "      values = self.ac.value(obs)\n",
    "      value_loss = (returns - values) ** 2\n",
    "      value_loss = value_loss.mean()\n",
    "\n",
    "      value_loss.backward()\n",
    "      self.value_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Return discounted rewards based on the given rewards and gamma param.\n",
    "    \"\"\"\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(float(rewards[i]) + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
    "    \"\"\"\n",
    "    Return the General Advantage Estimates from the given rewards and values.\n",
    "    Paper: https://arxiv.org/pdf/1506.02438.pdf\n",
    "    \"\"\"\n",
    "    next_values = np.concatenate([values[1:], [0]])\n",
    "    deltas = [rew + gamma * next_val - val for rew, val, next_val in zip(rewards, values, next_values)]\n",
    "\n",
    "    gaes = [deltas[-1]]\n",
    "    for i in reversed(range(len(deltas)-1)):\n",
    "        gaes.append(deltas[i] + decay * gamma * gaes[-1])\n",
    "\n",
    "    return np.array(gaes[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout.\n",
    "    Returns training data in the shape (n_steps, observation_shape)\n",
    "    and the cumulative reward.\n",
    "    \"\"\"\n",
    "    ### Create data storage\n",
    "    train_data = [[], [], [], [], []] # obs, act, reward, values, act_log_probs\n",
    "    obs = env.reset()[0]\n",
    "\n",
    "    ep_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        logits, val = model(torch.tensor([obs], dtype=torch.float32,\n",
    "                                         device=DEVICE))\n",
    "        act_distribution = Categorical(logits=logits)\n",
    "        act = act_distribution.sample()\n",
    "        act_log_prob = act_distribution.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val.item()\n",
    "\n",
    "        next_obs, reward, done, _, _ = env.step(act if isinstance(env.action_space,Discrete) else [act])\n",
    "\n",
    "        for i, item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "          train_data[i].append(item)\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_data = [np.asarray(x) for x in train_data]\n",
    "\n",
    "    ### Do train data filtering\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Purvansh Sonthalia\\AppData\\Local\\Temp\\ipykernel_10532\\4062642420.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  logits, val = model(torch.tensor([obs], dtype=torch.float32,\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(game_name,render_mode = 'human')\n",
    "model = ActorCriticNetwork(env.observation_space.shape[0], env.action_space)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "train_data, reward = rollout(model, env,max_steps=10) # Test rollout function\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training params\n",
    "n_episodes = 500\n",
    "print_freq = 1\n",
    "save_freq = 1\n",
    "max_steps = 500\n",
    "\n",
    "ppo = PPOTrainer(\n",
    "    model,\n",
    "    policy_lr = 3e-4,\n",
    "    value_lr = 1e-3,\n",
    "    target_kl_div = 0.02,\n",
    "    max_policy_train_iters = 40,\n",
    "    value_train_iters = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "if(os.path.isfile(f'{game_name}.pt')):\n",
    "    model.load_state_dict(torch.load(f'{game_name}.pt'))\n",
    "    print('loaded')\n",
    "else:\n",
    "    print('not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Avg Reward 1.0\n",
      "Episode 2 | Avg Reward -159.9\n",
      "Episode 3 | Avg Reward -47.6\n",
      "Episode 4 | Avg Reward 20.0\n",
      "Episode 5 | Avg Reward -109.7\n",
      "Episode 6 | Avg Reward -122.1\n",
      "Episode 7 | Avg Reward -24.9\n",
      "Episode 8 | Avg Reward -49.9\n",
      "Episode 9 | Avg Reward -11.8\n",
      "Episode 10 | Avg Reward -22.2\n",
      "Episode 11 | Avg Reward -93.9\n",
      "Episode 12 | Avg Reward -11.5\n",
      "Episode 13 | Avg Reward -139.1\n",
      "Episode 14 | Avg Reward -62.4\n",
      "Episode 15 | Avg Reward -141.2\n",
      "Episode 16 | Avg Reward 75.9\n",
      "Episode 17 | Avg Reward -137.1\n",
      "Episode 18 | Avg Reward -360.5\n",
      "Episode 19 | Avg Reward -363.0\n",
      "Episode 20 | Avg Reward -67.2\n",
      "Episode 21 | Avg Reward -253.7\n",
      "Episode 22 | Avg Reward -171.1\n",
      "Episode 23 | Avg Reward -138.1\n",
      "Episode 24 | Avg Reward -111.7\n",
      "Episode 25 | Avg Reward -324.7\n",
      "Episode 26 | Avg Reward -183.7\n",
      "Episode 27 | Avg Reward -76.1\n",
      "Episode 28 | Avg Reward -88.6\n",
      "Episode 29 | Avg Reward -69.4\n",
      "Episode 30 | Avg Reward -88.6\n",
      "Episode 31 | Avg Reward -225.4\n",
      "Episode 32 | Avg Reward -85.0\n",
      "Episode 33 | Avg Reward -333.5\n",
      "Episode 34 | Avg Reward -142.5\n",
      "Episode 35 | Avg Reward -277.6\n",
      "Episode 36 | Avg Reward -511.0\n",
      "Episode 37 | Avg Reward -159.6\n",
      "Episode 38 | Avg Reward -131.7\n",
      "Episode 39 | Avg Reward 12.1\n",
      "Episode 40 | Avg Reward 33.7\n",
      "Episode 41 | Avg Reward -148.4\n",
      "Episode 42 | Avg Reward -165.7\n",
      "Episode 43 | Avg Reward 126.9\n",
      "Episode 44 | Avg Reward -196.1\n",
      "Episode 45 | Avg Reward -28.4\n",
      "Episode 46 | Avg Reward -177.4\n",
      "Episode 47 | Avg Reward -24.8\n",
      "Episode 48 | Avg Reward 9.7\n",
      "Episode 49 | Avg Reward -141.0\n",
      "Episode 50 | Avg Reward 10.3\n",
      "Episode 51 | Avg Reward -93.3\n",
      "Episode 52 | Avg Reward -92.8\n",
      "Episode 53 | Avg Reward -81.9\n",
      "Episode 54 | Avg Reward -102.2\n",
      "Episode 55 | Avg Reward -41.6\n",
      "Episode 56 | Avg Reward -166.6\n",
      "Episode 57 | Avg Reward -14.1\n",
      "Episode 58 | Avg Reward -160.1\n",
      "Episode 59 | Avg Reward -45.9\n",
      "Episode 60 | Avg Reward -76.3\n",
      "Episode 61 | Avg Reward 8.0\n",
      "Episode 62 | Avg Reward -162.9\n",
      "Episode 63 | Avg Reward 12.0\n",
      "Episode 64 | Avg Reward -132.2\n",
      "Episode 65 | Avg Reward -43.7\n",
      "Episode 66 | Avg Reward -70.7\n",
      "Episode 67 | Avg Reward -102.6\n",
      "Episode 68 | Avg Reward -9.3\n",
      "Episode 69 | Avg Reward -106.3\n",
      "Episode 70 | Avg Reward -198.7\n",
      "Episode 71 | Avg Reward -33.4\n",
      "Episode 72 | Avg Reward -89.4\n",
      "Episode 73 | Avg Reward -44.9\n",
      "Episode 74 | Avg Reward -181.3\n",
      "Episode 75 | Avg Reward -65.1\n",
      "Episode 76 | Avg Reward 8.3\n",
      "Episode 77 | Avg Reward 4.4\n",
      "Episode 78 | Avg Reward -118.2\n",
      "Episode 79 | Avg Reward -123.7\n",
      "Episode 80 | Avg Reward -62.5\n",
      "Episode 81 | Avg Reward -97.1\n",
      "Episode 82 | Avg Reward -199.7\n",
      "Episode 83 | Avg Reward -6.4\n",
      "Episode 84 | Avg Reward 17.6\n",
      "Episode 85 | Avg Reward -91.3\n",
      "Episode 86 | Avg Reward -94.8\n",
      "Episode 87 | Avg Reward -26.8\n",
      "Episode 88 | Avg Reward -65.6\n",
      "Episode 89 | Avg Reward -84.7\n",
      "Episode 90 | Avg Reward -169.9\n",
      "Episode 91 | Avg Reward -79.3\n",
      "Episode 92 | Avg Reward -55.8\n",
      "Episode 93 | Avg Reward -61.4\n",
      "Episode 94 | Avg Reward -6.3\n",
      "Episode 95 | Avg Reward -109.3\n",
      "Episode 96 | Avg Reward -80.1\n",
      "Episode 97 | Avg Reward -79.3\n",
      "Episode 98 | Avg Reward -94.1\n",
      "Episode 99 | Avg Reward -152.6\n",
      "Episode 100 | Avg Reward -130.6\n",
      "Episode 101 | Avg Reward -46.0\n",
      "Episode 102 | Avg Reward -122.1\n",
      "Episode 103 | Avg Reward -66.1\n",
      "Episode 104 | Avg Reward -108.5\n",
      "Episode 105 | Avg Reward -149.5\n",
      "Episode 106 | Avg Reward -85.0\n",
      "Episode 107 | Avg Reward -141.9\n",
      "Episode 108 | Avg Reward -138.1\n",
      "Episode 109 | Avg Reward -169.6\n",
      "Episode 110 | Avg Reward -161.2\n",
      "Episode 111 | Avg Reward -121.0\n",
      "Episode 112 | Avg Reward -120.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m ep_rewards \u001b[39m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m episode_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_episodes):\n\u001b[0;32m      3\u001b[0m   \u001b[39m# Perform rollout\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m   train_data, reward \u001b[39m=\u001b[39m rollout(model, env, max_steps\u001b[39m=\u001b[39;49mmax_steps)\n\u001b[0;32m      5\u001b[0m   ep_rewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m      7\u001b[0m   \u001b[39m# Shuffle\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mrollout\u001b[1;34m(model, env, max_steps)\u001b[0m\n\u001b[0;32m     17\u001b[0m act_log_prob \u001b[39m=\u001b[39m act_distribution\u001b[39m.\u001b[39mlog_prob(act)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m act, val \u001b[39m=\u001b[39m act\u001b[39m.\u001b[39mitem(), val\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 21\u001b[0m next_obs, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(act \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(env\u001b[39m.\u001b[39;49maction_space,Discrete) \u001b[39melse\u001b[39;49;00m [act])\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m((obs, act, reward, val, act_log_prob)):\n\u001b[0;32m     24\u001b[0m   train_data[i]\u001b[39m.\u001b[39mappend(item)\n",
      "File \u001b[1;32mc:\\Users\\Purvansh Sonthalia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\Purvansh Sonthalia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Purvansh Sonthalia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\Purvansh Sonthalia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:675\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    672\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m+\u001b[39m\u001b[39m100\u001b[39m\n\u001b[0;32m    674\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 675\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(state, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32), reward, terminated, \u001b[39mFalse\u001b[39;00m, {}\n",
      "File \u001b[1;32mc:\\Users\\Purvansh Sonthalia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\box2d\\lunar_lander.py:728\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    726\u001b[0m         scaled_poly\u001b[39m.\u001b[39mappend((coord[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m SCALE, coord[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m SCALE))\n\u001b[0;32m    727\u001b[0m     pygame\u001b[39m.\u001b[39mdraw\u001b[39m.\u001b[39mpolygon(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msurf, (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), scaled_poly)\n\u001b[1;32m--> 728\u001b[0m     gfxdraw\u001b[39m.\u001b[39;49maapolygon(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msurf, scaled_poly, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))\n\u001b[0;32m    730\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparticles \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrawlist:\n\u001b[0;32m    731\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39mfixtures:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ep_rewards = []\n",
    "for episode_idx in range(n_episodes):\n",
    "  # Perform rollout\n",
    "  train_data, reward = rollout(model, env, max_steps=max_steps)\n",
    "  ep_rewards.append(reward)\n",
    "\n",
    "  # Shuffle\n",
    "  permute_idxs = np.random.permutation(len(train_data[0]))\n",
    "\n",
    "  # Policy data\n",
    "  obs = torch.tensor(train_data[0][permute_idxs],\n",
    "                     dtype=torch.float32, device=DEVICE)\n",
    "  acts = torch.tensor(train_data[1][permute_idxs],\n",
    "                      dtype=torch.int32, device=DEVICE)\n",
    "  gaes = torch.tensor(train_data[3][permute_idxs],\n",
    "                      dtype=torch.float32, device=DEVICE)\n",
    "  act_log_probs = torch.tensor(train_data[4][permute_idxs],\n",
    "                               dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "  # Value data\n",
    "  returns = discount_rewards(train_data[2])[permute_idxs]\n",
    "  returns = torch.tensor(returns, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "  # Train model\n",
    "  ppo.train_policy(obs, acts, act_log_probs, gaes)\n",
    "  ppo.train_value(obs, returns)\n",
    "\n",
    "  model.trained_for += 1\n",
    "\n",
    "  if (episode_idx + 1) % save_freq == 0:\n",
    "    torch.save(model.state_dict(), f'{game_name}.pt')\n",
    "\n",
    "  if (episode_idx + 1) % print_freq == 0:\n",
    "    print('Episode {} | Avg Reward {:.1f}'.format(\n",
    "        episode_idx + 1, np.mean(ep_rewards[-print_freq:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained agent\n",
    "model.load_state_dict(torch.load(f'{game_name}.pt'))\n",
    "print(f'Trained for {model.trained_for}')\n",
    "while(True):\n",
    "  # Perform rollout\n",
    "  train_data, reward = rollout(model, env)\n",
    "  env.render()\n",
    "\n",
    "  print('Episode {} | Avg Reward {:.1f}'.format(\n",
    "        episode_idx + 1, reward))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
